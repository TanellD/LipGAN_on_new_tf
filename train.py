from os import listdir, path
import numpy as np
import scipy
import cv2
import os, sys
import discriminator as md
import generator as mg
import keras
from keras.callbacks import ModelCheckpoint, Callback
from keras.utils import plot_model
from tqdm import tqdm
from glob import glob
import pickle, argparse
from keras.optimizers import Adam
from tensorflow.keras.losses import MSE
half_window_size = 4
mel_step_size = 27
import tensorflow as tf
from tensorflow.python.client import device_lib
import psutil
import gc


# @tf.function(input_signature=[
#     tf.TensorSpec(shape=[batch_size, img_size, img_size, 6], dtype=tf.float32),  # dummy_faces (input images)
#     tf.TensorSpec(shape=[batch_size, 80, mel_step_size, 1], dtype=tf.float32),   # audio (spectrogram)
#     tf.TensorSpec(shape=[batch_size, img_size, img_size, 3], dtype=tf.float32),  # real_faces (ground truth)
#     tf.TensorSpec(shape=[batch_size, img_size, img_size, 3], dtype=tf.float32)   # fake_faces (generated images)
# ])
# def train_step(dummy_faces, audio, real_faces, fake_faces):
#     # Forward pass through the discriminator to calculate the real/fake logits
#     with tf.GradientTape() as tape:
#         # Discriminator's prediction for real and fake faces
#         real_logits = disc([real_faces, audio], training=True)  # Real data (ground truth)
#         fake_logits = disc([fake_faces, audio], training=True)  # Fake data (generated by the generator)

#         # Calculate the contrastive loss
#         real_loss = contrastive_loss(tf.ones_like(real_logits), real_logits)  # Real should be 1
#         fake_loss = contrastive_loss(tf.zeros_like(fake_logits), fake_logits)  # Fake should be 0
        
#         # Total discriminator loss (sum of real and fake losses)
#         disc_loss = (real_loss + fake_loss) / 2

#     # Compute gradients of the loss with respect to discriminator weights
#     grads = tape.gradient(disc_loss, disc.trainable_weights)

#     # Apply gradients to update discriminator's weights
#     optimizer.apply_gradients(zip(grads, disc.trainable_weights))

#     # Now let's compute the generator's loss using the combined model (comb)
#     with tf.GradientTape() as gen_tape:
#         # Generator's forward pass to produce fake faces
#         gen_fakes = gen([dummy_faces, audio], training=True)

#         # Discriminator evaluates the generated fake faces
#         fake_logits_for_gen = disc([gen_fakes, audio], training=False)

#         # Generator loss consists of two components: 
#         # - Adversarial loss (we want the discriminator to classify fake faces as real)
#         # - L2 loss (to improve the quality of generated images)
#         gen_adv_loss = contrastive_loss(tf.ones_like(fake_logits_for_gen), fake_logits_for_gen)  # Fake faces should be classified as real

#         # L2 loss for generator (mean squared error between real and generated images)
#         gen_l2_loss = MSE(real_faces, gen_fakes)

#         # Total generator loss (sum of adversarial loss and L2 loss)
#         gen_loss = gen_adv_loss + gen_l2_loss

#     # Compute gradients for the generator
#     gen_grads = gen_tape.gradient(gen_loss, gen.trainable_weights)

#     # Apply the generator's gradients
#     optimizer.apply_gradients(zip(gen_grads, gen.trainable_weights))

#     return {
#         "disc_loss": disc_loss,
#         "gen_loss": gen_loss,
#         "gen_adv_loss": gen_adv_loss,
#         "gen_l2_loss": gen_l2_loss
#     }

# optimizer = keras.optimizers.SGD(learning_rate=1e-3)

def contrastive_loss(y_true, y_pred):
    margin = 1.
    loss = (1. - y_true) * tf.square(y_pred) + y_true * tf.square(tf.maximum(0., margin - y_pred))
    return tf.reduce_mean(loss)

def frame_id(fname):
	return int(os.path.basename(fname).split('.')[0])

def choose_ip_frame(frames, gt_frame):
	selected_frames = [f for f in frames if np.abs(frame_id(gt_frame) - frame_id(f)) >= 6]
	return np.random.choice(selected_frames)

def get_audio_segment(center_frame, spec):
	center_frame_id = frame_id(center_frame)
	start_frame_id = center_frame_id - half_window_size

	start_idx = int((80./25.) * start_frame_id) # 25 is fps of LRS2
	end_idx = start_idx + mel_step_size

	return spec[:, start_idx : end_idx] if end_idx <= spec.shape[1] else None

half_window_size = 4
mel_step_size = 27

# Helper functions
def frame_id(fname):
    return int(os.path.basename(fname).split('.')[0])

def choose_ip_frame(frames, gt_frame):
    selected_frames = [f for f in frames if np.abs(frame_id(gt_frame) - frame_id(f)) >= 6]
    return np.random.choice(selected_frames)

def get_audio_segment(center_frame, spec):
    center_frame_id = frame_id(center_frame)
    start_frame_id = center_frame_id - half_window_size
    start_idx = int((80. / 25.) * start_frame_id)  # 25 is fps of LRS2
    end_idx = start_idx + mel_step_size
    
    # Extract the segment from the spectrogram
    mel_segment = spec[:, start_idx:end_idx] if end_idx <= spec.shape[1] else None
    
    
    return mel_segment

# Generator function for tf.data.Dataset
class DataGenerator(object):
	def __init__(self, all_images, batch_size, img_size, mel_step_size):

		self.img_gt_batch = []
		self.img_ip_batch = []
		self.mel_batch = []
		self.all_images = all_images
		self.batch_size = batch_size
		self.img_size = img_size
		self.mel_step_size = mel_step_size
		self.i = -1
		self.prev_comleted_batch = None

	def next(self):
		
		self.img_ip_batch.clear()
		self.mel_batch.clear()
		self.img_gt_batch.clear()
		img_gt_batch_arr, img_ip_batch_arr, mel_batch_arr, img_gt_batch_masked_arr = None, None, None, None
		del img_gt_batch_arr, img_ip_batch_arr, mel_batch_arr, img_gt_batch_masked_arr
		while len(self.img_gt_batch) < self.batch_size:	
			self.i += 1
			self.i = self.i % len(self.all_images)
		# For each image in the batch
			img_name = self.all_images[self.i]
			gt_fname = os.path.basename(img_name)
			dir_name = img_name.replace(gt_fname, '')
			frames = glob(dir_name + '/*.jpg')

			if len(frames) < 12:
				continue

			mel_fname = dir_name + '/mels.npz'
			try:
				mel = np.load(mel_fname)['spec']
			except:
				continue

			mel = get_audio_segment(gt_fname, mel)

			if mel is None or mel.shape[1] != self.mel_step_size:
				continue

			if np.isnan(mel.flatten()).sum() > 0:
				continue	

			img_gt = cv2.imread(img_name)
			img_gt = cv2.resize(img_gt, (self.img_size, self.img_size))

			ip_fname = choose_ip_frame(frames, gt_fname)
			img_ip = cv2.imread(ip_fname)
			img_ip = cv2.resize(img_ip, (self.img_size, self.img_size))

			self.img_gt_batch.append(img_gt)
			self.img_ip_batch.append(img_ip)
			self.mel_batch.append(mel)

		if len(self.img_gt_batch) < self.batch_size:
			yield self.prev_comleted_batch	
		img_gt_batch_arr = np.asarray(self.img_gt_batch)
		img_ip_batch_arr = np.asarray(self.img_ip_batch)
		mel_batch_arr = np.expand_dims(np.asarray(self.mel_batch), 3)
		img_gt_batch_masked_arr = img_gt_batch_arr.copy()
		img_gt_batch_masked_arr[:, self.img_size//2:,...] = 0.
		img_ip_batch_arr = np.concatenate([img_ip_batch_arr, img_gt_batch_masked_arr], axis=3)
		if self.prev_comleted_batch is not None:
			del self.prev_comleted_batch
		self.prev_comleted_batch = (img_ip_batch_arr.copy() / 255.0, mel_batch_arr.copy()), img_gt_batch_arr.copy() / 255.0
		yield (img_ip_batch_arr / 255.0, mel_batch_arr), img_gt_batch_arr / 255.0
		
	
	def __next__(self):
		return self.next()
	def __iter__(self):
		return self

	

# Create a tf.data.Dataset from the generator
# def create_tf_dataset(all_images, batch_size, img_size, mel_step_size):
#     dataset = tf.data.Dataset.from_generator(
#         lambda: data_generator(all_images, batch_size, img_size, mel_step_size),
#         output_signature=(
#             (tf.TensorSpec(shape=(batch_size, img_size, img_size, 6), dtype=tf.float32),   # Input images (img_ip)
#              tf.TensorSpec(shape=(batch_size, 80, mel_step_size, 1), dtype=tf.float32)), # Mel spectrogram (audio)
#             tf.TensorSpec(shape=(batch_size, img_size, img_size, 3), dtype=tf.float32)     # Ground truth images (real_faces)
#         )
#     )
#     dataset = dataset.prefetch(tf.data.AUTOTUNE)  # Prefetch to improve performance
#     return dataset

# Assuming you already have a list of image file paths
with open('logs/filenames.pkl', 'rb') as f:
    all_images = pickle.load(f)

# Parameters
batch_size = 32
img_size = 96
mel_step_size = 27

# Create TensorFlow Dataset
train_dataset = DataGenerator(all_images, batch_size, img_size, mel_step_size)


parser = argparse.ArgumentParser(description='Keras implementation of LipGAN')

parser.add_argument('--data_root', type=str, help='LRS2 preprocessed dataset root to train on', required=True)
parser.add_argument('--logdir', type=str, help='Folder to store checkpoints & generated images', default='logs/')

parser.add_argument('--model', type=str, help='Model name to use: basic|residual', default='residual')
parser.add_argument('--resume_gen', help='Path to weight file to load into the generator', default=None)
parser.add_argument('--resume_disc', help='Path to weight file to load into the discriminator', default=None)
parser.add_argument('--checkpoint_freq', type=int, help='Frequency of checkpointing', default=1000)

parser.add_argument('--n_gpu', type=int, help='Number of GPUs to use', default=1)
parser.add_argument('--batch_size', type=int, help='Single GPU batch size', default=batch_size)
parser.add_argument('--lr', type=float, help='Initial learning rate', default=1e-4)
parser.add_argument('--img_size', type=int, help='Size of input image', default=96)
parser.add_argument('--epochs', type=int, help='Number of epochs', default=10)

parser.add_argument('--all_images', default='filenames.pkl', help='Filename for caching image paths')
args = parser.parse_args()

if path.exists(path.join(args.logdir, args.all_images)):
	args.all_images = pickle.load(open(path.join(args.logdir, args.all_images), 'rb'))
else:
	all_images = glob(path.join("{}/train/*/*/*.jpg".format(args.data_root)))
	pickle.dump(all_images, open(path.join(args.logdir, args.all_images), 'wb'), protocol=pickle.HIGHEST_PROTOCOL)
	args.all_images = all_images

print ("Will be training on {} images".format(len(args.all_images)))

if args.model == 'residual':
	gen = mg.create_model_residual(args, mel_step_size)
else:
	gen = mg.create_model(args, mel_step_size)

disc = md.create_model(args, mel_step_size)
comb = mg.create_combined_model(gen, disc, args, mel_step_size)
if args.resume_gen:
	gen.load_weights(args.resume_gen)
	print('Resuming generator from : {}'.format(args.resume_gen))
if args.resume_disc:
	disc.load_weights(args.resume_disc)
	print('Resuming discriminator from : {}'.format(args.resume_disc))

args.batch_size = args.n_gpu * args.batch_size
# train_datagen = datagen(args)

comb.summary()
disc.compile(optimizer=Adam(learning_rate=args.lr), loss=contrastive_loss)
disc.trainable = True
# for e in range(args.epochs):
# 	# print("1.5,disc-loss:",disc._loss_tracker)
# 	prog_bar = tqdm(range(len(args.all_images) // args.batch_size), total=len(args.all_images) // args.batch_size)
# 	disc_loss, unsync_loss, sync_loss, gen_loss_mae, gen_loss_adv = 0., 0., 0., 0., 0.
# 	prog_bar.set_description('Starting epoch {}'.format(e))
# 	batch_idx = 0
# 	for batch_idx in prog_bar:
# 		# print(type(train_dataset))
# 		# print(next(train_dataset))
# 		print(psutil.virtual_memory(), 'for next')
# 		(dummy_faces, audio), real_faces = list(next(train_dataset))[0]
# 		dummy_faces = tf.convert_to_tensor(dummy_faces, dtype='float32')
# 		audio = tf.convert_to_tensor(audio, dtype='float32')
# 		real_faces = tf.convert_to_tensor(real_faces, dtype='float32')
# 		print(psutil.virtual_memory(), 'for start')
# 		# (dummy_faces, audio), real_faces = datagen(args, batch_idx)  # Get the next batch of data
# 		real = np.zeros((len(real_faces), 1))  # Real labels for discriminator
# 		fake = np.ones((len(real_faces), 1))  # Fake labels for discriminator
# 		print(psutil.virtual_memory(), 'for mid')
# 		fake = tf.convert_to_tensor(fake, dtype='float32')
# 		real = tf.convert_to_tensor(real, dtype='float32')
# 		# Convert dummy_faces, audio, and fake labels to TensorFlow tensors
# 		print(psutil.virtual_memory(), 'for gen fakes before')
# 		gen_fakes = gen([dummy_faces, audio]) ### possible area to improove
# 		print(psutil.virtual_memory(), 'for gen fakes after')
# 		gen_fakes = tf.convert_to_tensor(gen_fakes)
# 		print('-------------------')
# 		print(dummy_faces.dtype)
# 		print(audio.dtype)
# 		print(real_faces.dtype)
# 		print(gen_fakes.dtype)
# 		print(fake.dtype)
# 		print(real.dtype)
# 		print('-------------------')
# 		# disc.predict()
# 		### Train discriminator
# 		# print("2,disc-loss:",disc._loss_tracker)
# 		# disc.trainable = True
# 		print(psutil.virtual_memory(), 'for mid')

# 		# @tf.function
# 		# def train_step(x, y):
# 		# 	with tf.GradientTape() as tape:
# 		# 		logits = disc(x, training=True)
# 		# 		loss_value = contrastive_loss(y, logits)
# 		# 		# Add any extra losses created during the forward pass.
# 		# 		loss_value += sum(disc.losses)
# 		# 	grads = tape.gradient(loss_value, disc.trainable_weights)
# 		# 	optimizer.apply_gradients(zip(grads, disc.trainable_weights))
# 		# 	print(loss_value.numpy(), 'check')
# 		# 	return loss_value.numpy()
# 		#### all within a random ifelse can be improoved
# 		if np.random.choice([True, False]):
# 			disc_loss += disc.train_step([[gen_fakes, audio], fake])['loss']
# 			unsync_loss += disc.evaluate([real_faces, tf.convert_to_tensor(np.roll(audio, 10, axis=0), dtype='float32')], fake)
# 		else:
# 			disc_loss += disc.evaluate([gen_fakes, audio], fake)
# 			unsync_loss += disc.train_step([[real_faces, tf.convert_to_tensor(np.roll(audio, 10, axis=0), dtype='float32')], fake])['loss']
# 		print(psutil.virtual_memory(), 'for random')
# 		sync_loss += disc.train_on_batch([real_faces, audio], real) ## also can be improoved
# 		print(psutil.virtual_memory(), 'for train on batch disc')
# 		### Train generator
# 		# disc.trainable = False
# 		total, mae, adv = comb.train_on_batch([dummy_faces, audio], [real_faces, real]) ### should be improoved
# 		print(psutil.virtual_memory(), 'for train on batch comb')
# 		gen_loss_mae += mae
# 		gen_loss_adv += adv

# 		prog_bar.set_description('\nDisc_loss: {}, Unsynced: {}, Synced: {}, MAE: {} Adv_loss: {}'.format(\
# 														round(disc_loss / (batch_idx + 1), 3), 
# 														round(unsync_loss / (batch_idx + 1), 3), 
# 														round(sync_loss / (batch_idx + 1), 3),
# 														round(gen_loss_mae / (batch_idx + 1), 3),
# 														round(gen_loss_adv / (batch_idx + 1), 3)))
# 		prog_bar.refresh()

# 		if (batch_idx + 1) % (args.checkpoint_freq // 10) == 0:
# 			if (batch_idx + 1) % args.checkpoint_freq == 0:
# 				disc.save(path.join(args.logdir, 'disc.h5'))
# 				gen.save(path.join(args.logdir, 'gen.h5'))
# 				comb.save(path.join(args.logdir, 'comb.h5'))

# 			collage = np.concatenate([dummy_faces[...,:3], real_faces, gen_fakes], axis=2)
# 			collage *= 255.
# 			collage = np.clip(collage, 0., 255.).astype(np.uint8)
			
# 			for i in range(len(collage)):
# 				cv2.imwrite(path.join(args.logdir, 'gen_faces/{}.jpg'.format(i)), collage[i])
# 		print(psutil.virtual_memory(), 'before deleting')
# 		print( sys.getsizeof(dummy_faces) +sys.getsizeof(real_faces) +sys.getsizeof(gen_fakes) + sys.getsizeof(audio))
# 		del dummy_faces, real_faces, gen_fakes, audio
# 		gc.collect()
# 		print(psutil.virtual_memory(), 'after deleting')
# 		# batch_idx += 1
# 		# if batch_idx > args.all_images // args.batch_size:
# 		# 	break
# 	tf.keras.backend.clear_session()


def generator_fn(all_images, batch_size, img_size, mel_step_size):
    data_gen = DataGenerator(all_images, batch_size, img_size, mel_step_size)
    for data in data_gen:
        yield list(data)[0]

# Create the tf.data.Dataset
def create_tf_dataset(all_images, batch_size, img_size, mel_step_size):
    dataset = tf.data.Dataset.from_generator(
        lambda: generator_fn(all_images, batch_size, img_size, mel_step_size),
        output_signature=(
            (tf.TensorSpec(shape=[batch_size, img_size, img_size, 6], dtype=tf.float32),  # img_ip_batch + img_gt_batch_masked
             tf.TensorSpec(shape=[batch_size, 80, mel_step_size, 1], dtype=tf.float32)),  # mel_batch
            tf.TensorSpec(shape=[batch_size, img_size, img_size, 3], dtype=tf.float32)  # img_gt_batch
        )
    )
    return dataset

dataset = create_tf_dataset(all_images, batch_size, img_size, mel_step_size)


comb.fit(dataset.take(1900), epochs=9, steps_per_epoch=1900)

comb.generator.save_weights(f"logs/trained_generator.weights.h5")

